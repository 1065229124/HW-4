---
title: "Homework Assignment 4"
author: "Johnny Ji (9425752)"
output:
  html_document:
    df_print: paged
latex_engine: xelatex
---

```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
tidymodels_prefer()

knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r}
titanic <- read_csv("data/titanic.csv")
set.seed(2022)

titanic$survived =  factor(titanic$survived, levels = c("Yes", "No")) 
titanic$pclass =  factor(titanic$pclass)

```

##Q1


```{r}

titanic_split <- initial_split(titanic, strata = survived, prop = 0.8)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
dim(titanic_train)
dim(titanic_test)
```

We can tell number of observations by using dim().


```{r}

titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>% 
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~ starts_with("sex"):age + age:fare)

```

##Q2


```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
```


##Q3





*In Q2, we split training titanic sample into 10 small samples, one of unique group as a hold out or test data set, and remaining groups as a training data set, then each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times, at last, k results  be averaged to produce a single estimation.*

*The advantage of this method is to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model, and, each observation is used for validation exactly once.*

*If we used the entire training set that would be a validation approach.*





##Q4


```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```



$3*10 = 30$, will fit 30 models.


##Q5


```{r}
log_fit <- fit_resamples(log_wkflow, titanic_folds)
lda_fit <- fit_resamples(lda_wkflow, titanic_folds)
qda_fit <- fit_resamples(qda_wkflow, titanic_folds)
```


##Q6


```{r}
collect_metrics(log_fit)
collect_metrics(lda_fit)
collect_metrics(qda_fit)

```
From above, we can tell that logistic regression model performed the best. It has the highest mean accuracy and smallest standard error  of the two other model.



##Q7


```{r}
Best_fit <- fit(log_wkflow, titanic_train)
```


##Q8


```{r}
titanic_testing_pred <- 
  predict(Best_fit, titanic_test) %>% 
  bind_cols(titanic_test %>% select(survived))%>% 
  bind_cols(predict(Best_fit, titanic_test, type = "prob")) %>% 
  accuracy(truth = survived, .pred_class)

titanic_testing_pred
```


Therefore, the testing accuracy is close to the average accuracy across folds, so the k-fold cross-validation method fits well.


##Q9



$\sum_{i=1}^n (y_{i} - \beta - \epsilon)^2$

then we get

$\sum_{i=1}^n (y_{i} - \beta)^2$

take derivative

$2\sum_{i=1}^n (y_{i} - \beta)=0$

$n\hat\beta=\sum_{i=1}^n y_i$

At last, we get that 

$\hat\beta=\frac{\sum_{i=1}^n y_i}{n}$


##Q10

we can get that 

$\hat{\beta}^{(1)}=\frac{\sum_{i=2}^n y_i}{n-1}$

$\hat{\beta}^{(2)}=\frac{y_1+\sum_{i=3}^n y_i}{n-1}$

Since they have uncorrelated error, which means 
$Cov(y_i,y_j)=var(y_i)$ if i=j, $Cov(y_i,y_j)=0$ if $i\neq j$  

Thus, $Cov(\hat\beta_1,\hat\beta_2)=Cov(\frac{\sum_{i=2}^n y_i}{n-1},\frac{y_1+\sum_{i=3}^n y_i}{n-1})=\frac{Cov(\sum_{i=2}^n y_i,y_1+\sum_{i=3}^n y_i)} {n-1}=\frac{(n-2)\sigma^2}{n-1}$